/*首先，运行以下命令以从PyTorch中导出参考数据：
$ python layernorm.py
然后，编译并运行C版本：
$ g++ layernorm.cpp -o layernorm -lm
$ ./layernorm*/

/*Summary
代码中使用了C++11特性,实现了一个C++类 LayerNorm，用于执行层归一化（Layer Normalization）操作，
在karpathy大佬的layernorm.c基础上构建了C++版本，实现了前向传播和反向传播的功能，
并通过Python生成的参考数据进行了验证。其中layernorm.py中的文件被写为float64格式，以支持C++中的double类型。
lncpp.bin文件是Python生成的参考数据，layernorm.cpp是C++代码，layernorm.py是Python代码，layernorm.c是参考的源码。

The code uses C++11 features to implement a C++ class LayerNorm for performing Layer Normalization operations.
Based on the layernorm.c of karpathy, a C++ version is built to implement the forward and backward propagation functions,
and verified with the reference data generated by Python. The file in layernorm.py is written in float64 format to support the double type in C++.
lncpp.bin is the reference data generated by Python, layernorm.cpp is the C++ code, layernorm.py is the Python code, and layernorm.c is the reference source code.
*/

#include <cstdio>
#include <cstdlib>
#include <cmath>
#include <cstring>
#include <string>
#include <fstream>
#include <iostream>
#include <vector>
#include <memory>

using namespace std;

class LayerNorm {
public:
    void layernorm_forward(double* out,double* mean, double* rstd,
                       double* inp, double* weight, double* bias,
                       int B, int T, int C);
    void layernorm_backward(double* dinp, double* dweight, double* dbias,
                         double* dout, double* inp, double* weight, double* mean, double* rstd,
                         int B, int T, int C);
    int check_tensor(double* a, double* b, int n, string label);    
};

void LayerNorm::layernorm_forward(double* out,double* mean, double* rstd,
                                   double* inp, double* weight, double* bias,
                                   int B, int T, int C) {
    double eps = 1e-5;
    for (int b = 0; b < B; b++) {
        for (int t = 0; t < T; t++) {
            // seek to the input position inp[b,t,:]
            // 寻找输入位置 inp[b,t，：]
            double* x = inp + b * T * C + t * C;
            // calculate the mean
            // 计算平均值
            double m = 0.0;
            for (int i = 0; i < C; i++) {
                m += x[i];
            }
            m = m/C;
            // calculate the variance (without any bias correction)
            // 计算方差（不带任何偏差校正）
            double v = 0.0;
            for (int i = 0; i < C; i++) {
                double xshift = x[i] - m;
                v += xshift * xshift;
            }
            v = v/C;
            // calculate the rstd
            // 计算标准差倒数
            double s = 1.0 / sqrtf(v + eps);
            // seek to the output position in out[b,t,:]
            // 寻找输出位置 out[b,t，：]
            double* out_bt = out + b * T * C + t * C;
            for (int i = 0; i < C; i++) {
                double n = (s * (x[i] - m)); // normalized output 标准化输出
                double o = n * weight[i] + bias[i]; // scale and shift it 缩放和移位
                out_bt[i] = o; // write the output 写入输出
            }
            // cache the mean and rstd for the backward pass later 
            // 为后面的反向传播缓存平均值和标准差倒数
            mean[b * T + t] = m;
            rstd[b * T + t] = s;
        }
    }
}

void LayerNorm::layernorm_backward(double* dinp, double* dweight, double* dbias,
                         double* dout, double* inp, double* weight, double* mean, double* rstd,
                         int B, int T, int C) {
     for (int b = 0; b < B; b++) {
        for (int t = 0; t < T; t++) {
            double* dout_bt = dout + b * T * C + t * C;
            double* inp_bt = inp + b * T * C + t * C;
            double* dinp_bt = dinp + b * T * C + t * C;
            double mean_bt = mean[b * T + t];
            double rstd_bt = rstd[b * T + t];

            // first: two reduce operations
            // 第一步：两个减少操作
            double dnorm_mean = 0.0;
            double dnorm_norm_mean = 0.0;
            for (int i = 0; i < C; i++) {
                double norm_bti = (inp_bt[i] - mean_bt) * rstd_bt;
                double dnorm_i = weight[i] * dout_bt[i];
                dnorm_mean += dnorm_i;
                dnorm_norm_mean += dnorm_i * norm_bti;
            }
            dnorm_mean = dnorm_mean / C;
            dnorm_norm_mean = dnorm_norm_mean / C;

            // now iterate again and accumulate all the gradients
            // 现在再次迭代并累积所有梯度
            for (int i = 0; i < C; i++) {
                double norm_bti = (inp_bt[i] - mean_bt) * rstd_bt;
                double dnorm_i = weight[i] * dout_bt[i];
                // gradient contribution to bias
                // 偏差的梯度贡献
                dbias[i] += dout_bt[i];
                // gradient contribution to weight
                // 权重的梯度贡献
                dweight[i] += norm_bti * dout_bt[i];
                // gradient contribution to input
                // 输入的梯度贡献
                double dval = 0.0;
                dval += dnorm_i; // term 1
                dval -= dnorm_mean; // term 2
                dval -= norm_bti * dnorm_norm_mean; // term 3
                dval *= rstd_bt; // final scale
                dinp_bt[i] += dval;
            }
        }
    }
};

// poor man's tensor checker
// 简陋的张量检查器
int LayerNorm::check_tensor(double *a, double *b, int n, string label) {
    int ok = 1;
    cout << label << endl;
    for (int i = 0; i < n; i++) {
        if (fabs(a[i] - b[i]) <= 1e-5) {
            printf("OK ");
        } else {
            printf("NOT OK ");
            ok = 0;
        }
        printf("%f %f\n", a[i], b[i]);
    }
    return ok;
}

int main() {
    LayerNorm layernorm;

    int B = 2; // batch
    int T = 3; // time / sequence length
    int C = 4; // number of channels

    unique_ptr<double[]> x (new double[B * T * C * sizeof(double)]);
    unique_ptr<double[]> w (new double[C * sizeof(double)]);
    unique_ptr<double[]> b (new double[C * sizeof(double)]);
    unique_ptr<double[]> out (new double[B * T * C * sizeof(double)]);
    unique_ptr<double[]> mean (new double[B * T * sizeof(double)]);
    unique_ptr<double[]> rstd (new double[B * T * sizeof(double)]);
    unique_ptr<double[]> dout (new double[B * T * C * sizeof(double)]);
    unique_ptr<double[]> dx (new double[B * T * C * sizeof(double)]);
    unique_ptr<double[]> dw (new double[C * sizeof(double)]);
    unique_ptr<double[]> db (new double[C * sizeof(double)]);

    // read reference information from Python
    // 从Python读取参考信息
    ifstream file("lncpp.bin", ios::binary);
    if (!file) {
        std::cerr << "Error opening file" << std::endl;
        return 1;
    }

    file.read(reinterpret_cast<char*>(x.get()), sizeof(double) * B * T * C);
    file.read(reinterpret_cast<char*>(w.get()), sizeof(double) * C);
    file.read(reinterpret_cast<char*>(b.get()), sizeof(double) * C);
    file.read(reinterpret_cast<char*>(out.get()), sizeof(double) * B * T * C);
    file.read(reinterpret_cast<char*>(mean.get()), sizeof(double) * B * T);
    file.read(reinterpret_cast<char*>(rstd.get()), sizeof(double) * B * T);
    file.read(reinterpret_cast<char*>(dout.get()), sizeof(double) * B * T * C);
    file.read(reinterpret_cast<char*>(dx.get()), sizeof(double) * B * T * C);
    file.read(reinterpret_cast<char*>(dw.get()), sizeof(double) * C);
    file.read(reinterpret_cast<char*>(db.get()), sizeof(double) * C);

    if (!file) {
        std::cerr << "Error reading file" << std::endl;
        return 1;
    }

    // No need to close the file as it will be closed automatically when file goes out of scope
    // 当文件超出范围时，不需要关闭文件，因为它将自动关闭
    // now let's calculate everything ourselves
    //  现在让我们自己计算所有内容
    // forward pass
    // 前向传递
    unique_ptr<double[]> c_out (new double[B * T * C * sizeof(double)]);
    unique_ptr<double[]> c_mean (new double[B * T * sizeof(double)]);
    unique_ptr<double[]> c_rstd (new double[B * T * sizeof(double)]);
    layernorm.layernorm_forward(c_out.get(), c_mean.get(), c_rstd.get(), x.get(), w.get(), b.get(), B, T, C);

    // check correctness of forward pass
    layernorm.check_tensor(out.get(), c_out.get(), B*T*C, "out");
    layernorm.check_tensor(mean.get(), c_mean.get(), B*T, "mean");
    layernorm.check_tensor(rstd.get(), c_rstd.get(), B*T, "rstd");

    // backward pass (note calloc inits grads to zero)
    unique_ptr<double[]> c_dx(new double[B * T * C]);
    std::fill_n(c_dx.get(), B * T * C, 0.0); // 初始化为0.0

    unique_ptr<double[]> c_dw(new double[B * T * C]);
    std::fill_n(c_dw.get(), B * T, 0.0); // 初始化为0.0

    unique_ptr<double[]> c_db(new double[B * T * C]);
    std::fill_n(c_db.get(), B * T, 0.0); // 初始化为0.0

    layernorm.layernorm_backward(c_dx.get(), c_dw.get(), c_db.get(), dout.get(), x.get(), w.get(), c_mean.get(), c_rstd.get(), B, T, C);

    // check correctness of backward pass
    layernorm.check_tensor(c_dx.get(), dx.get(), B*T*C, "dx");
    layernorm.check_tensor(c_dw.get(), dw.get(), C, "dw");
    layernorm.check_tensor(c_db.get(), db.get(), C, "db");

    return 0;
}